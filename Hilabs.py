# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SFerN6buj0IJ9VUwcgG0wPhxQLFOihqM
"""


# If the model/repo you pull requires HF authentication, run this and paste your token.
from huggingface_hub import login
HF_TOKEN = "hf_xxxxxxxxxxxxxxxxxxxxxxxxxxx"  # <<< optionally paste token here between quotes, or leave blank to not login
if HF_TOKEN:
    login(HF_TOKEN)
    print("Logged in to Hugging Face")
else:
    print("No HF token provided — proceed (public repos only).")

# Cell: load local model (4-bit)
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# Model IDs (change if you want another quantized repo)
MODEL_ID_QUANT = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit"   # pre-quantized 4bit (community repo)
TOKENIZER_ID = "mistralai/Mistral-7B-Instruct-v0.3"           # tokenizer from official repo (recommended)

print("torch cuda available:", torch.cuda.is_available())
device = "cuda" if torch.cuda.is_available() else "cpu"

# BitsAndBytes quant config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_ID, use_fast=False)

print("Loading 4-bit quantized model (this may take ~1-5 min depending on GPU/network)...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID_QUANT,
    device_map="auto",                  # automatically spreads layers to GPUs / CPU if needed
    quantization_config=bnb_config,     # use 4-bit config (transformers >= 4.31 supports BitsAndBytesConfig)
    trust_remote_code=True,             # community repos sometimes require this flag
    use_auth_token=HF_TOKEN or None     # if repo is gated and you logged in above
)

model.eval()
print("Model loaded. Device map:", model.hf_device_map if hasattr(model, "hf_device_map") else "N/A")

# Small wrapper for generation
import torch

def generate_local(prompt: str, max_new_tokens: int = 256, temperature: float = 0.0, top_p: float = 0.95):
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}
    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=(temperature > 0.0),
            temperature=temperature,
            top_p=top_p,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            early_stopping=True
        )
    decoded = tokenizer.decode(out[0], skip_special_tokens=True)
    # If the model echoes the prompt, return only the completion after the prompt
    if decoded.startswith(prompt):
        return decoded[len(prompt):].strip()
    return decoded.strip()

# Quick smoke test
prompt = "Write a short SQL query: <SQL>SELECT 1;</SQL>"
print("Generating (smoke test)...")
print(generate_local(prompt, max_new_tokens=50))

from pickle import NONE
# NL2SQL pipeline + Gradio UI (local model)
import re, sqlite3, pandas as pd
import gradio as gr

# helper: build schema string from DataFrame columns (simple type guesses)
def columns_to_schema_string(df: pd.DataFrame) -> str:
    def guess_type(series: pd.Series) -> str:
        if pd.api.types.is_integer_dtype(series):
            return "int"
        if pd.api.types.is_float_dtype(series):
            return "float"
        return "text"
    parts = []
    for col in df.columns:
        parts.append(f"{col} ({guess_type(df[col])})")
    return ", ".join(parts)

def build_nl2sql_prompt(schema_str: str, user_question: str) -> str:
    prompt = (
        "You are an expert NL2SQL assistant. Given a table named `data` with the schema below, "
        "and a natural language question, produce a single SQLite-compatible SQL query that answers the question.\n\n"
        "Schema (table name: data):\n"
        f"{schema_str}\n\n"
        "Important instructions:\n"
        " - Output ONLY the SQL statement and nothing else. Surround the SQL exactly with <SQL> and </SQL> tags, e.g. <SQL>SELECT ...;</SQL>\n"
        " - Use standard SQLite SQL syntax (LIMIT, ORDER BY, JOIN, GROUP BY, etc.).\n"
        " - Assume the table is named `data` and contains the CSV rows.\n\n"
        f"User question: {user_question}\n\n"
        "Respond now with only: <SQL>...your SQL here...</SQL>"
    )
    return prompt

def extract_sql_from_model_output(text: str) -> str:
    tag_match = re.search(r"<SQL>(.*?)</SQL>", text, re.S | re.I)
    if tag_match:
        return tag_match.group(1).strip()
    sel_match = re.search(r"(SELECT[\s\S]*?;)", text, re.I)
    if sel_match:
        return sel_match.group(1).strip()
    # fallback
    return text.strip()

def safe_sql_check(sql: str) -> bool:
    s = sql.strip().lower()
    s_lead = s.split(None, 1)[0] if s else ""
    allowed_prefixes = {"select", "with", "pragma", "explain"}
    forbidden_keywords = ["delete", "update", "insert", "drop", "alter", "attach", "detach", "create", "replace"]
    if s_lead not in allowed_prefixes:
        return False
    for kw in forbidden_keywords:
        if re.search(r"\b" + re.escape(kw) + r"\b", s):
            return False
    return True

def process_csv_and_question_local(csv_file, user_question: str):
    if csv_file is None:
        return "No CSV uploaded.", pd.DataFrame()
    try:
        df = pd.read_csv(csv_file.name)
    except Exception as e:
        return f"Failed to read CSV: {e}", pd.DataFrame()
    schema_str = columns_to_schema_string(df)
    prompt = build_nl2sql_prompt(schema_str, user_question)
    # call LOCAL model
    try:
        model_out = generate_local(prompt, max_new_tokens=256, temperature=0.0)
    except Exception as e:
        return f"Local model generation error: {e}", pd.DataFrame()
    sql = extract_sql_from_model_output(model_out)
    if not safe_sql_check(sql):
        return f"Model produced unsafe/unsupported SQL:\n{sql}", pd.DataFrame()
    # run with sqlite in-memory
    try:
        conn = sqlite3.connect(":memory:")
        df.to_sql("data", conn, index=False, if_exists="replace")
    except Exception as e:
        return f"Failed to create sqlite DB: {e}\nSQL produced:\n{sql}", pd.DataFrame()
    try:
        cur = conn.cursor()
        cur.execute(sql)
        cols = [d[0] for d in cur.description] if cur.description else []
        rows = cur.fetchall()
        result_df = pd.DataFrame(rows, columns=cols) if rows else pd.DataFrame(columns=cols)
        conn.close()
        return sql, result_df
    except Exception as e:
        return f"SQL execution error: {e}\nSQL produced:\n{sql}", pd.DataFrame()

# Build Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## NL2SQL (local Mistral Instruct, 4-bit in Colab)\nUpload CSV, ask question — model runs locally on the Colab GPU.")
    with gr.Row():
        csv_in = gr.File(label="Upload CSV file", file_types=[".csv"])
        question_in = gr.Textbox(label="Natural language question", lines=2, placeholder="e.g. Show top 5 customers by total_sales")
    sql_out = gr.Textbox(label="Generated SQL (from local model)", lines=4)
    result_out = gr.Dataframe(headers=None, label="Query result")
    btn = gr.Button("Run NL2SQL (local)")
    def gradio_fn(csv_file, q):
        sql_or_msg, df_res = process_csv_and_question_local(csv_file, q)
        if isinstance(df_res, pd.DataFrame):
            return sql_or_msg, df_res
        else:
            return sql_or_msg, pd.DataFrame()
    btn.click(gradio_fn, inputs=[csv_in, question_in], outputs=[sql_out, result_out])
demo.launch(share=True)